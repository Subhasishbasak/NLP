{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Word_recognition_NN.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Subhasishbasak/NLP/blob/master/Word_recognition_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So2kizZ9d5kO",
        "colab_type": "text"
      },
      "source": [
        "## Natural_Language_Processing/Assignment_2/MDS201803"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reHF29dyylrQ",
        "colab_type": "text"
      },
      "source": [
        "Link to the documentation [Click here](https://drive.google.com/open?id=1eEH_WuUeuXkzcQBBJsc9YKwdVAPzPJYn)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJtgIpzod5kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic Imports\n",
        "\n",
        "import numpy as np\n",
        "import string\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ifROY1Hd5kn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary = [\"about\", \"above\",\"abuse\",\"added\",\"adult\",\"after\",\"again\",\"agent\",\"agree\",\n",
        "             \"beach\", \"began\", \"begin\", \"being\", \"below\", \"bible\", \"bills\", \"birds\", \"birth\",\n",
        "             \"calls\", \"cards\", \"carry\", \"cases\", \"catch\", \"cause\", \"cells\", \"chain\", \"chair\",\n",
        "             \"dates\", \"death\", \"depth\", \"doing\", \"doors\", \"doubt\", \"draft\", \"drawn\",\"dream\",\n",
        "             \"eight\", \"email\", \"empty\", \"ended\", \"enemy\", \"enjoy\", \"enter\", \"entry\", \"equal\",\n",
        "             \"facts\", \"faith\", \"feels\", \"fewer\", \"field\", \"fifth\", \"fight\", \"filed\", \"files\",\n",
        "             \"gifts\", \"girls\", \"given\", \"gives\", \"glass\", \"goals\", \"going\", \"goods\", \"grade\",\n",
        "             \"hands\", \"happy\", \"heads\", \"heard\", \"heart\", \"heavy\", \"hello\", \"helps\", \"hence\",\n",
        "             \"ideal\", \"ideas\", \"image\", \"index\", \"inner\", \"input\", \"issue\", \"items\", \"joint\",\n",
        "             \"judge\", \"juice\", \"keeps\", \"kinds\", \"knife\", \"known\", \"knows\", \"label\", \"labor\",\n",
        "             \"large\", \"later\", \"layer\", \"leads\", \"learn\", \"least\", \"leave\", \"legal\", \"level\",\n",
        "             \"magic\", \"major\", \"makes\", \"match\", \"maybe\", \"meals\", \"means\", \"meant\", \"media\",\n",
        "             \"never\", \"newly\", \"night\", \"noise\", \"north\", \"noted\", \"notes\", \"nurse\", \"occur\",\n",
        "             \"older\", \"opens\", \"order\", \"other\", \"owned\", \"owner\", \"pages\", \"paint\",\"panel\", \"paper\", \"parks\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiGK1JgKd5kw",
        "colab_type": "text"
      },
      "source": [
        "#### One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvc6cydzd5kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function for generating One-Hot vector encoding of the English words & Alphabets\n",
        "\n",
        "def one_hot_encode(w):\n",
        "    \n",
        "    output = np.zeros(26)\n",
        "    test_list = list(string.ascii_lowercase) \n",
        "    \n",
        "    for i in list(w):\n",
        "        index = test_list.index(i)\n",
        "        output[index] = output[index] + 1\n",
        "    \n",
        "    # takes the average of the one hot vectors of each alphabet of the word\n",
        "    output = output/5\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oCge-p8d5k7",
        "colab_type": "text"
      },
      "source": [
        "#### Creating the Input matrix of shape $128 \\times 26$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX_quL5Md5k9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following code stacks all the input One hot vectors and constructs the input matrix\n",
        "\n",
        "X = None\n",
        "for i in vocabulary:\n",
        "    x = one_hot_encode(i)\n",
        "    try:\n",
        "        X = np.row_stack([X, x])\n",
        "    except ValueError:\n",
        "        X = x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLX8K1LCd5lF",
        "colab_type": "text"
      },
      "source": [
        "#### Creating the output matrix of shape $128 \\times 128$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK2NZVSKd5lH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.identity(128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P4-1ciQd5lN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function learns parameters for the neural network and returns the model.\n",
        "\n",
        "#  iterations : Number of passes through the training data for gradient descent\n",
        "#  print_loss: If True, print the loss every 1000 iterations\n",
        "\n",
        "def Neural_network(hidden_dim, iterations):\n",
        "    \n",
        "    input_dim = 26           # input layer dimensionality\n",
        "    hidden_dim = 10          # hidden layer dimensionality\n",
        "    output_dim = 128         # output layer dimensionality\n",
        "\n",
        "    # parameters Gradient descent \n",
        "\n",
        "    eta = 0.01           # learning rate for gradient descent\n",
        "\n",
        "    # First we initialize the parameters to random values. \n",
        "    np.random.seed(0)\n",
        "    W1 = np.random.randn(input_dim, hidden_dim)    # First weight matrix of shape 26 x 10 \n",
        "    b1 = np.zeros((1, hidden_dim))                 # bias vector of shape 10 x 1\n",
        "    W2 = np.random.randn(hidden_dim, output_dim)   # Second weight matrix of shape 10 x 128\n",
        "    b2 = np.zeros((1, output_dim))                 # bias vector of shape 128 x 1\n",
        "\n",
        "    # We build the Neural Network model as a dictionary storing the parameters (Weights & Biases respectively)\n",
        "    param_dict = {}\n",
        "\n",
        "    # We use the Gradient descent algorithm to estimate the parameters by updating each batch\n",
        "    for i in range(0, iterations):\n",
        "\n",
        "        # Forward propagation\n",
        "        \n",
        "        z1 = X.dot(W1) + b1          # z1 obtained by multiplyng weight W1 and adding bias b1 with input\n",
        "        a1 = np.tanh(z1)             # a1 obtained after applying tanh() activation to z1\n",
        "        z2 = a1.dot(W2) + b2         # z2 obtained by multiplyng weight W2 and adding bias b2 with a1\n",
        "        exp_scores = np.exp(z2)           \n",
        "        sftmax = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # softmax scores to generate probabilities\n",
        "\n",
        "        # Backpropagation\n",
        "        \n",
        "        delta3 = sftmax - y            # taking difference of observed any predicted node values\n",
        "        dW2 = (a1.T).dot(delta3)\n",
        "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
        "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
        "        dW1 = np.dot(X.T, delta2)\n",
        "        db1 = np.sum(delta2, axis=0)\n",
        "\n",
        "        # Gradient descent parameter update\n",
        "        \n",
        "        W1 = W1 - eta * dW1\n",
        "        b1 = b1 - eta * db1\n",
        "        W2 = W2 - eta * dW2\n",
        "        b2 = b2 - eta * db2\n",
        "\n",
        "        # Updating the model with new parameters\n",
        "        \n",
        "        param_dict = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
        "\n",
        "    return param_dict\n",
        "                  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5n46Yied5lV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to predict the output\n",
        "\n",
        "def predict(param_dict, x):\n",
        "    \n",
        "    # takes the weights and biases from the trained model\n",
        "    \n",
        "    W1, b1, W2, b2 = param_dict['W1'], param_dict['b1'], param_dict['W2'], param_dict['b2']\n",
        "    \n",
        "    # Forward propagation\n",
        "    \n",
        "    z1 = x.dot(W1) + b1\n",
        "    a1 = np.tanh(z1)\n",
        "    z2 = a1.dot(W2) + b2\n",
        "    exp_scores = np.exp(z2)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "    \n",
        "    return np.argmax(probs, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nu9z5X6d5lc",
        "colab_type": "text"
      },
      "source": [
        "#### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK6HK8G-d5le",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Neural_network(10, 500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cggfBGKwd5lk",
        "colab_type": "text"
      },
      "source": [
        "Using the accuracy function we compute the accuracy of the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWscmSlDd5lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to compute the accuracy of the model\n",
        "# To check Training accuracy keep, train = True\n",
        "# To check Test accuracy keep, train = False, test = input dictionary\n",
        "# To print the false positive results keep the \"print_false\" flag on\n",
        "\n",
        "def accuracy(model, train = True, test = None, print_false = True):\n",
        "\n",
        "    if train:\n",
        "        count = 0\n",
        "        for i in vocabulary:\n",
        "            if predict(model, one_hot_encode(i))[0] == vocabulary.index(i):\n",
        "                count += 1\n",
        "            else:\n",
        "                if print_false:\n",
        "                    print(\"input word : %s, predicted word : %s, predicted index : %i, actual index : %i\" %(i, vocabulary[predict(model, one_hot_encode(i))[0]], predict(model, one_hot_encode(i))[0], vocabulary.index(i)))\n",
        "        return count/len(vocabulary)\n",
        "        \n",
        "    else:\n",
        "        count = 0\n",
        "        for i in list(test.keys()):\n",
        "            if vocabulary[predict(model, one_hot_encode(i))[0]] == test[i]:\n",
        "                count += 1\n",
        "                if print_false:\n",
        "                    print(\"input word : %s, predicted word : %s, expected word : %s, CORRECT PREDICTION\" %(i, vocabulary[predict(model, one_hot_encode(i))[0]], test[i]))\n",
        "            else:\n",
        "                if print_false:\n",
        "                    print(\"input word : %s, predicted word : %s, expected word : %s\" %(i, vocabulary[predict(model, one_hot_encode(i))[0]], test[i]))\n",
        "            \n",
        "        return count/len(list(test.keys()))\n",
        "    \n",
        "                  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTPJa5_id5ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function is used to plot the accuracy values corresponding the iteration values\n",
        "# This function finds the iteration size for which the two consecutive accuracies differ no more than 0.00001 such that\n",
        "# the accuracy is at least greater than some threshold or iteration number is less than some threshold\n",
        "\n",
        "def plot_accuracy(hdim , max_acc = 0.99, max_iter = 2500):\n",
        "    \n",
        "    acc_list = []\n",
        "    itr_list = []\n",
        "    iteration = 1\n",
        "    acc_1 = accuracy(Neural_network(hdim, iteration), train = True, print_false= False)\n",
        "    acc_2 = accuracy(Neural_network(hdim, iteration + 50), train = True, print_false= False)\n",
        "\n",
        "    while abs(acc_1-acc_2)>0.00001 or acc_1 < 0.99 and iteration < 2500:\n",
        "\n",
        "        iteration += 50\n",
        "        acc_1 = accuracy(Neural_network(hdim, iteration), train = True, print_false= False)\n",
        "        acc_2 = accuracy(Neural_network(hdim, iteration + 50), train = True, print_false= False)\n",
        "        acc_list.append(acc_1)\n",
        "        itr_list.append(iteration)\n",
        "        \n",
        "    print(\"accuracy achieved : \",acc_1)\n",
        "    print(\"best iteration :\", iteration) \n",
        "    \n",
        "    return acc_list, itr_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXXpLesPd5l3",
        "colab_type": "code",
        "outputId": "37531b52-3f94-4b19-eaed-ce44452d6eb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "accuracy_list, iteration_list = plot_accuracy(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy achieved :  0.984375\n",
            "best iteration : 2501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-GwFSHjd5l-",
        "colab_type": "text"
      },
      "source": [
        "Thus we see that the best accuracy achieved after 2500 iteration (i.e. 2500 batch gradiant descent updates) is $98.43 \\%$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZlY7EtFd5mA",
        "colab_type": "code",
        "outputId": "2a36e3c4-060a-4043-9f6f-4e97da80fee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "# Plotting Training accuracy w.r.t iteration \n",
        "\n",
        "fig = plt.figure()\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"training accuracy\")\n",
        "plt.plot(iteration_list,accuracy_list, '-')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd955838ac8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk3c4a6sd5mG",
        "colab_type": "text"
      },
      "source": [
        "From above we can see that the accuracy improves till $500$ iterations and almost stops improving after $700-1000$ iterations. This graph helps us to identify a suitable iteration value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf_TOrhMd5mJ",
        "colab_type": "code",
        "outputId": "7225d720-7a4a-4dad-c997-618f3843d43a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "accuracy(model, train = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input word : being, predicted word : begin, predicted index : 11, actual index : 12\n",
            "input word : field, predicted word : filed, predicted index : 52, actual index : 49\n",
            "input word : ideas, predicted word : ended, predicted index : 39, actual index : 73\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9765625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hcKnzkVd5mO",
        "colab_type": "text"
      },
      "source": [
        "We train the Neural Network with $10$ nodes in the hidden layer and for $500$ iterations. After some trial and parameter tuning we found that the training accuracy of the model is $97.65 \\%$. The words it misclassifies are as above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YNamiqQd5mP",
        "colab_type": "text"
      },
      "source": [
        "#### Model Testing with mis-spelled words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btY9qnnld5mS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dict = {\"ajent\" : \"agent\",\n",
        "             \"abouu\" : \"about\", \n",
        "             \"indes\" : \"index\", \n",
        "             \"abain\" : \"again\", \n",
        "             \"layer\" : \"later\", \n",
        "             \"aappy\" : \"happy\",\n",
        "             \"march\" : \"match\",\n",
        "             \"majik\" : \"magic\",\n",
        "             \"layre\" : \"layer\",\n",
        "             \"gless\" : \"glass\",\n",
        "             \"billo\" : \"bills\",\n",
        "             \"other\" : \"othre\",\n",
        "             \"juict\" : \"juice\",\n",
        "             \"feelz\" : \"feels\",\n",
        "             \"knofe\" : \"knife\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0OpFEAnd5mW",
        "colab_type": "code",
        "outputId": "23df45cc-a744-49e1-b7d3-ab7aa19c5376",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "accuracy(model, train = False, test = test_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input word : ajent, predicted word : paint, expected word : agent\n",
            "input word : abouu, predicted word : about, expected word : about, CORRECT PREDICTION\n",
            "input word : indes, predicted word : field, expected word : index\n",
            "input word : abain, predicted word : again, expected word : again, CORRECT PREDICTION\n",
            "input word : layer, predicted word : layer, expected word : later\n",
            "input word : aappy, predicted word : happy, expected word : happy, CORRECT PREDICTION\n",
            "input word : march, predicted word : match, expected word : match, CORRECT PREDICTION\n",
            "input word : majik, predicted word : again, expected word : magic\n",
            "input word : layre, predicted word : layer, expected word : layer, CORRECT PREDICTION\n",
            "input word : gless, predicted word : girls, expected word : glass\n",
            "input word : billo, predicted word : bills, expected word : bills, CORRECT PREDICTION\n",
            "input word : other, predicted word : other, expected word : othre\n",
            "input word : juict, predicted word : juice, expected word : juice, CORRECT PREDICTION\n",
            "input word : feelz, predicted word : feels, expected word : feels, CORRECT PREDICTION\n",
            "input word : knofe, predicted word : knife, expected word : knife, CORRECT PREDICTION\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nNESM8Hd5mb",
        "colab_type": "text"
      },
      "source": [
        "As we can see our trained model is able to predict the correct words for the mis-spelled inputs. We used a test dataset of size $15$ mis-spelled words. The test accuracy obtained was $60 \\%$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJWL4hd6d5mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}